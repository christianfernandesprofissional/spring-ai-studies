spring.application.name=SpringAI
logging.pattern.console=%green(%d{HH:mm:ss.SSS}) %blue(%-5level) %red([%thread]) %yellow(%logger{15}) - %msg%n
spring.ai.ollama.chat.model=llama3.1

spring.ai.ollama.embedding.model=nomic-embed-text


#Configuração para autorizar os logs das requisições para LLM
logging.leve.org.springframework.ai.cha.client.advisor=DEBUG

#Podemos passar ChatOptions através do application.properties
#spring.ai.chat.options.---- #após.options. temos diversas opções de configurações

#h2 config
# jdbc:h2:file:~/chatmemory;AUTO_SERVER=true <- Com esta config sera criado um arquivo chamado chatmemory com os dados armazenados
spring.datasource.url=jdbc:h2:file:C:\\chatmemory;AUTO_SERVER=true 
spring.datasource.driver-class-name=org.h2.Driver
spring.datasource.username=llmchat
spring.datasource.password=12345

spring.ai.chat.memory.repository.jdbc.initialize-schema=always
#Dizendo ao Spring onde ele pode encontrar o schema SQL para o banco
spring.ai.chat.memory.repository.jdbc.schema=classpath:/schema/schema-h2db.sql


spring.docker.compose.stop.command=down

#Config QDrant

#Se a collection ainda não existir no Qdrant, crie automaticamente
spring.ai.vectorstore.qdrant.initialize-schema=true
#Define onde o Qdrant está rodando
spring.ai.vectorstore.qdrant.host=localhost
#Define a porta do Qdrant
spring.ai.vectorstore.qdrant.port=6334
#Define o nome da collection(nome equivalente a tabela no QDrant) onde os vetores serão armazenados
spring.ai.vectorstore.qdrant.collection-name=eazybytes